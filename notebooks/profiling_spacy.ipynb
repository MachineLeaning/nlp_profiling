{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling NLP: optimizing code with spaCy\n",
    "\n",
    "## Objective\n",
    "\n",
    "To compare two Python natural-language processing libraries with respect to their speed and efficiency\n",
    "\n",
    "## Architecture\n",
    "\n",
    "NLTK is very popular but not optimized for speed. SpaCy is built with Cython, which gives it an enormous speed advantage, but many people misuse it in a way that slows it down to Python speeds. We'll examine ways to achieve common NLP tasks while avoiding time overhead.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "The processing pipeline(s) include:\n",
    "\n",
    "    NLTK version:\n",
    "      - tokenize: split texts into individual tokens\n",
    "      - lowercase: normalize the vocabulary by case\n",
    "      - stopword removal: remove tokens if they appear in a specified list\n",
    "      - tag: tag part of speech (for lemmatization)\n",
    "      - lemmatize: normalize the vocabulary to the base form of each token\n",
    "      - join (optional): return the list of tokens in one joined string\n",
    "      \n",
    "The standard spaCy pipeline includes all of this, plus dependency parsing and NER:\n",
    "      \n",
    "    spaCy version:\n",
    "      - tokenize\n",
    "      - tag: tag part of speech\n",
    "      - parse: perform dependency parsing\n",
    "      - named entity recognition: extract named entities according to statistical model\n",
    "      - lemmatize\n",
    "      - join (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "# import gensim\n",
    "from itertools import dropwhile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputError(AttributeError):\n",
    "    '''if parameter spacy == True, corpus input must be of type spacy.tokens.doc.Doc, not {doc.__class__}!\\ne.g.: clean_text(nlp(\"You keep using that word. I do not think it means what you think it means.\")'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc, spacy=True, printed=False, list_tokens=False):\n",
    "    '''\n",
    "    define a simple preprocessing pipeline for general NLP tasks.\n",
    "    non-spaCy version:\n",
    "      - tokenize: split texts into individual tokens\n",
    "      - lowercase: normalize the vocabulary by case\n",
    "      - stopword removal: remove tokens if they appear in a specified list\n",
    "      - tag: tag part of speech (for lemmatization)\n",
    "      - lemmatize: normalize the vocabulary to the base form of each token\n",
    "      - join (optional): return the list of tokens in one joined string\n",
    "      \n",
    "      the spaCy pipeline includes all of this, plus dependency parsing and NER:\n",
    "      \n",
    "    spaCy version: (we will disable some of these pipes in testing.)\n",
    "      - tokenize\n",
    "      - lemmatize\n",
    "      - stopword removal\n",
    "      - tag: tag part of speech\n",
    "      - parse: perform dependency parsing\n",
    "      - named entity recognition: extract named entities according to statistical model\n",
    "      - join (optional)\n",
    "    '''  \n",
    "\n",
    "    if spacy:\n",
    "        try: \n",
    "            doc = [token.lemma_ for token in doc if not token.is_stop and not token.pos_ in ['PRON', 'PUNCT']]\n",
    "            #dropwhile(lambda x: not (x.is_stop and x.pos_ in ['PRON', 'PUNCT']), tokens)\n",
    "            #if not token.is_stop and not token.pos_ in set(['PRON', 'PUNCT'])\n",
    "            if not list_tokens:\n",
    "                return nlp.make_doc(' '.join([token for token in doc]))\n",
    "            else:\n",
    "                return doc\n",
    "        except AttributeError as ae:\n",
    "            print(f'''ERROR! if parameter spacy == True, corpus input must be of type spacy.tokens.doc.Doc, not {doc.__class__}!\\ne.g.: clean_text(nlp(\"You keep using that word. I do not think it means what you think it means.\")''')\n",
    "            raise\n",
    "\n",
    "    else:\n",
    "        '''\n",
    "        spaCy's default pipeline includes tokenizer + lemmatizer + POS-tagging\n",
    "        we've added stopword removal to both processes\n",
    "        '''\n",
    "        def pos_tag_nltk(token, printed=False):\n",
    "            tag_map = defaultdict(lambda : wn.NOUN)\n",
    "            tag_map['J'] = wn.ADJ\n",
    "            tag_map['V'] = wn.VERB\n",
    "            tag_map['R'] = wn.ADV\n",
    "        \n",
    "            nonlocal lemmatizer\n",
    "        \n",
    "            token, tag = zip(*pos_tag([token]))\n",
    "            lemma = lemmatizer.lemmatize(token[0], tag_map[tag[0][0]])\n",
    "            if printed:\n",
    "                print(token[0], \"=>\", lemma)\n",
    "            return lemma\n",
    "\n",
    "        new_doc = []\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        doc = tokenizer.tokenize(doc)\n",
    "        for token in doc:\n",
    "            if token.lower() not in stop_words:\n",
    "                new_doc.append(pos_tag_nltk(token.lower(), printed))\n",
    "        if not list_tokens:\n",
    "            return ' '.join([token for token in new_doc])\n",
    "        else:\n",
    "            return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'spacy.tokens.doc.Doc'>\nNLTK: let try ner barack obama germany 5 million ten clock part speech get lemmatized differently fun make thing make film make fun\n\nspaCy: let try ner Barack Obama Germany $ 5 million o'clock part speech lemmatize differently fun make thing making film make fun\n"
    }
   ],
   "source": [
    "doc_example = '''Let\\'s try some NER: Barack Obama, Germany, $5 million, ten o\\'clock. the parts of speech get lemmatized differently: it's fun making things, the making of the film, making fun of you'''\n",
    "\n",
    "a = '''This is a Doc. I have made one, and it\\'s clean now! Pretty cool, huh? It\\'s fun making docs in spaCy. \n",
    "               Let\\'s try for some NER: Barack Obama, Germany, $5 million, Quakers, ten o\\'clock.\n",
    "               Did you notice that made and make are stop words in spaCy but making isn't?\n",
    "               the parts of speech get lemmatized differently: it's fun making things, the making of the film, making fun of you\n",
    "               WordNet treats lemmas differently: repeat repeating repeats repeated repetition repetitive\n",
    "               in spaCy, clocks are different from o\\'clock '''\n",
    "\n",
    "'''\n",
    "Note the differences in spaCy's stopwords list\n",
    "and lemmatization vs. NLTK's. Also note that\n",
    "spaCy won't eliminate newline characters for you.\n",
    "'''\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nltk_ex = clean_text(doc_example, spacy=False)\n",
    "spacy_ex = clean_text(nlp(doc_example), spacy=True)\n",
    "\n",
    "print(f'NLTK: {nltk_ex}')\n",
    "print('')\n",
    "print(f'spaCy: {spacy_ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "let try ner Barack Obama Germany $ 5 million o'clock part speech lemmatize differently fun make thing making film make fun\n"
    }
   ],
   "source": [
    "spacy_ex = clean_text(nlp(doc_example))\n",
    "print(spacy_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making film\n",
      "make banana cream pie\n",
      "make fun dumb photo\n",
      "make light situation\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "However, spaCy's conservative lemmatization \n",
    "lets it keep the entities that it recognizes together,\n",
    "unlike NLTK, which does not keep them in one unit.\n",
    "\n",
    "spaCy also takes part of speech into account when lemmatizing.\n",
    "if syntactic nuance is important to your use case, this\n",
    "can be a valuable disambiguating tool.\n",
    "\n",
    "NLTK: ten, clock\n",
    "spaCy: ten o'clock\n",
    "\n",
    "NLTK:  making of + N (nominalized V) -> make\n",
    "       making + N (V, progressive aspect) -> make\n",
    "spaCy: making of + n -> making (retains form of nominalization)\n",
    "       making + N -> make \n",
    "\n",
    "NB: it's sensitive enough to know the difference between \n",
    "\"making of + N\" and \"making N of\"\n",
    "\n",
    "'''\n",
    "spacy_doc = nlp(doc_example)\n",
    "spacy_doc.ents\n",
    "\n",
    "print(clean_text(nlp('The making of the film')))\n",
    "print(clean_text(nlp('making a banana cream pie')))\n",
    "print(clean_text(nlp('making fun of your dumb photos')))\n",
    "print(clean_text(nlp('making light of the situation')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x1004a0a50>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x1f9058050>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x21bec89f0>)]\n"
    }
   ],
   "source": [
    "'''\n",
    "note that the order in which components are used counts.\n",
    "this ordering recognizes two kinds of Barack Obama,\n",
    "because he has been lowercased.\n",
    "'''\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.pipeline)\n",
    "\n",
    "docs = ['doc one', 'barack obama', 'fifty billion', 'Barack Obama']\n",
    "\n",
    "def nltk_clean_nlp(docs, spacy=False):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        clean_text(doc, spacy=False)\n",
    "        new_docs.append(doc)\n",
    "        \n",
    "        # if spacy == True:\n",
    "        #     doc = nlp(doc)\n",
    "        #     ents = doc.ents\n",
    "        #     print(doc, ents)\n",
    "    \n",
    "    return new_docs\n",
    "\n",
    "l = nltk_clean_nlp(docs, spacy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_docs(url='http://archives.textfiles.com/media.zip'): # http://archives.textfiles.com/media.zip\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()\n",
    "    prof_docs = []\n",
    "    for file in z.namelist():\n",
    "        tmp = z.extract(file)\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                prof_docs.append(f.read())\n",
    "        except Exception as e:\n",
    "            print(f'could not read {file}: {e}')\n",
    "    return prof_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "could not read media/EPISODES/: [Errno 21] Is a directory: 'media/EPISODES/'\ncould not read media/EPISODES/warnrlis.txt: 'utf-8' codec can't decode byte 0x8e in position 2443: invalid start byte\ncould not read media/SCRIPTS/: [Errno 21] Is a directory: 'media/SCRIPTS/'\ncould not read media/christop.int: 'utf-8' codec can't decode byte 0x97 in position 7661: invalid start byte\ncould not read media/earp: 'utf-8' codec can't decode byte 0xda in position 8: invalid continuation byte\ncould not read media/oliver.txt: 'utf-8' codec can't decode byte 0xb0 in position 1: invalid start byte\ncould not read media/oliver02.txt: 'utf-8' codec can't decode byte 0xb0 in position 1: invalid start byte\ncould not read media/widows: 'utf-8' codec can't decode byte 0xda in position 8: invalid continuation byte\n"
    },
    {
     "data": {
      "text/plain": "161"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof_docs = get_sample_docs()\n",
    "len(prof_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('reddit-comments-2015-08.csv')\n",
    "blog_docs = reddit.body.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling\n",
    "\n",
    "## Round 1: NLTK\n",
    "\n",
    "### Total time: 1897.58 s (that's 31.6 minutes)\n",
    "### 60.4%: clean_text\n",
    "### 39.1%: nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n*** Profile printout saved to text file 'nltk_reddit_2.txt'. \n"
    }
   ],
   "source": [
    "%lprun -f nltk_clean_nlp -f clean_text -T nltk_reddit_2.txt nltk_clean_nlp(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2: spaCy, the wrong way\n",
    "\n",
    "### (Converting a fast c struct to a slow python list)\n",
    "\n",
    "### Total time: 871.419  s (still half the time of NLTK)\n",
    "\n",
    "### 98.2%: list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x20e313dd0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x202c43ad0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x20e2a0de0>)]\n"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.pipeline)\n",
    "\n",
    "def list_pipe_docs(docs): \n",
    "    '''\n",
    "    For an apples-to-apples comparison, let's disable the\n",
    "    dependency parse and named entity recognition pipes, \n",
    "    since NLTK isn't doing those things.\n",
    "    '''\n",
    "    doc_convert = []\n",
    "    for doc in list(nlp.pipe(docs, disable=['parser', 'ner'])):\n",
    "        new_doc = clean_text(doc)\n",
    "        # for token in doc:\n",
    "        #     if not token.is_stop or token.pos_ in ['PRON', 'PUNCT']:\n",
    "        #         new_doc.append(token)\n",
    "        doc_convert.append(new_doc)\n",
    "    return doc_convert\n",
    "                \n",
    "        #print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n*** Profile printout saved to text file 'spacy_list_reddit_2.txt'. \n"
    }
   ],
   "source": [
    "%lprun -f list_pipe_docs -f clean_text -T spacy_list_reddit_2.txt list_pipe_docs(blog_docs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3: spaCy, using (some) Cython optimization\n",
    "\n",
    "### Total time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x20e30dbd0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x1a8ba5210>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x1a8ba59f0>), ('clean_text', <function clean_text at 0x21c719cb0>)]\n"
    }
   ],
   "source": [
    "nlp_pipeline = spacy.load('en_core_web_lg')\n",
    "nlp_pipeline.add_pipe(clean_text, 'clean_text')\n",
    "print(nlp_pipeline.pipeline)\n",
    "\n",
    "def pipe_docs(docs):\n",
    "    for doc in nlp_pipeline.pipe(docs, disable=['parser', 'ner']):\n",
    "        # _ = doc.text\n",
    "        yield doc\n",
    "\n",
    "def get_piped_docs(docs):\n",
    "    gen_docs = []\n",
    "    for doc in pipe_docs(docs):\n",
    "        gen_docs.append(doc)\n",
    "    return gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n*** Profile printout saved to text file 'spacy_pipe_reddit_2.txt'. \n"
    },
    {
     "data": {
      "text/plain": "'\\nclean_text is much faster in the pipe_docs function\\nthe slowest part is the string join to return a spacy doc at the end\\n'"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lprun -f get_piped_docs -f pipe_docs -f clean_text -T spacy_pipe_reddit_2.txt get_piped_docs(blog_docs)\n",
    "\n",
    "'''\n",
    "clean_text is much faster in the pipe_docs function\n",
    "the slowest part is the string join to return a spacy doc at the end\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Winner: spaCy pipeline\n",
    "\n",
    "* spaCy with Cython: 11 minutes\n",
    "* spaCy with Python lists: 14.5 minutes\n",
    "* NLTK: 31.6 minutes\n",
    "\n",
    "The slowest parts of the pipeline are still the Python parts: the list comprehension (23.6% of total time, 2821.8 ms) and the string join with list comprehension (76.3% of total time, 9118.1 ms). (This could be further optimized, too.)\n",
    "\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "* NLTK is a significantly slower way to preprocess text\n",
    "* Using spaCy's nlp in a for-loop is still better than NLTK, but not as good as nlp.pipe()\n",
    "* However, converting the nlp.pipe object to a Python list negates several minutes of time saved due to Cython efficiencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv-awesomenlp': pyenv)",
   "language": "python",
   "name": "python37464bitvenvawesomenlppyenv2e5736c03fa34e80a40180789394a341"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}