Timer unit: 1e-06 s

Total time: 60.3561 s
File: <ipython-input-105-ac264522bb96>
Function: pipe_docs at line 5

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     5                                           def pipe_docs(docs):
     6     15001   60347009.0   4022.9    100.0      for doc in nlp_pipeline.pipe(docs, disable=['parser', 'ner']):
     7                                                   # _ = doc.text
     8     15000       9107.0      0.6      0.0          yield doc

Total time: 60.3943 s
File: <ipython-input-105-ac264522bb96>
Function: get_piped_docs at line 10

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    10                                           def get_piped_docs(docs):
    11         1          2.0      2.0      0.0      gen_docs = []
    12     15001   60384132.0   4025.3    100.0      for doc in pipe_docs(docs):
    13     15000      10176.0      0.7      0.0          gen_docs.append(doc)
    14         1          1.0      1.0      0.0      return gen_docs

Total time: 4.50741 s
File: <ipython-input-96-b17dbd2e43df>
Function: clean_text at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def clean_text(doc, spacy=True, printed=False, list_tokens=False):
     2                                               '''
     3                                               define a simple preprocessing pipeline for general NLP tasks.
     4                                               non-spaCy version:
     5                                                 - tokenize: split texts into individual tokens
     6                                                 - lowercase: normalize the vocabulary by case
     7                                                 - stopword removal: remove tokens if they appear in a specified list
     8                                                 - tag: tag part of speech (for lemmatization)
     9                                                 - lemmatize: normalize the vocabulary to the base form of each token
    10                                                 - join (optional): return the list of tokens in one joined string
    11                                                 
    12                                                 the spaCy pipeline includes all of this, plus dependency parsing and NER:
    13                                                 
    14                                               spaCy version: (we will disable some of these pipes in testing.)
    15                                                 - tokenize
    16                                                 - lemmatize
    17                                                 - stopword removal
    18                                                 - tag: tag part of speech
    19                                                 - parse: perform dependency parsing
    20                                                 - named entity recognition: extract named entities according to statistical model
    21                                                 - join (optional)
    22                                               '''  
    23                                           
    24     15000      20107.0      1.3      0.4      if spacy:
    25     15000      18915.0      1.3      0.4          try: 
    26     15000    1296131.0     86.4     28.8              doc = [token.lemma_ for token in doc if not token.is_stop and not token.pos_ in ['PRON', 'PUNCT']]
    27                                                       #dropwhile(lambda x: not (x.is_stop and x.pos_ in ['PRON', 'PUNCT']), tokens)
    28                                                       #if not token.is_stop and not token.pos_ in set(['PRON', 'PUNCT'])
    29     15000      20266.0      1.4      0.4              if not list_tokens:
    30     15000    3151995.0    210.1     69.9                  return nlp.make_doc(' '.join([token for token in doc]))
    31                                                       else:
    32                                                           return doc
    33                                                   except AttributeError as ae:
    34                                                       print(f'''ERROR! if parameter spacy == True, corpus input must be of type spacy.tokens.doc.Doc, not {doc.__class__}!\ne.g.: clean_text(nlp("You keep using that word. I do not think it means what you think it means.")''')
    35                                                       raise
    36                                           
    37                                               else:
    38                                                   '''
    39                                                   spaCy's default pipeline includes tokenizer + lemmatizer + POS-tagging
    40                                                   we've added stopword removal to both processes
    41                                                   '''
    42                                                   def pos_tag_nltk(token, printed=False):
    43                                                       tag_map = defaultdict(lambda : wn.NOUN)
    44                                                       tag_map['J'] = wn.ADJ
    45                                                       tag_map['V'] = wn.VERB
    46                                                       tag_map['R'] = wn.ADV
    47                                                   
    48                                                       nonlocal lemmatizer
    49                                                   
    50                                                       token, tag = zip(*pos_tag([token]))
    51                                                       lemma = lemmatizer.lemmatize(token[0], tag_map[tag[0][0]])
    52                                                       if printed:
    53                                                           print(token[0], "=>", lemma)
    54                                                       return lemma
    55                                           
    56                                                   new_doc = []
    57                                                   tokenizer = RegexpTokenizer(r'\w+')
    58                                                   lemmatizer = WordNetLemmatizer()
    59                                                   doc = tokenizer.tokenize(doc)
    60                                                   for token in doc:
    61                                                       if token.lower() not in stop_words:
    62                                                           new_doc.append(pos_tag_nltk(token.lower(), printed))
    63                                                   if not list_tokens:
    64                                                       return ' '.join([token for token in new_doc])
    65                                                   else:
    66                                                       return new_doc