Timer unit: 1e-06 s

Total time: 64.2596 s
File: <ipython-input-101-cddc8dc710c6>
Function: list_pipe_docs at line 4

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     4                                           def list_pipe_docs(docs): 
     5                                               '''
     6                                               For an apples-to-apples comparison, let's disable the
     7                                               dependency parse and named entity recognition pipes, 
     8                                               since NLTK isn't doing those things.
     9                                               '''
    10         1          2.0      2.0      0.0      doc_convert = []
    11     15001   57958589.0   3863.6     90.2      for doc in list(nlp.pipe(docs, disable=['parser', 'ner'])):
    12     15000    6281200.0    418.7      9.8          new_doc = clean_text(doc)
    13                                                   # for token in doc:
    14                                                   #     if not token.is_stop or token.pos_ in ['PRON', 'PUNCT']:
    15                                                   #         new_doc.append(token)
    16     15000      19831.0      1.3      0.0          doc_convert.append(new_doc)
    17         1          4.0      4.0      0.0      return doc_convert

Total time: 6.04083 s
File: <ipython-input-96-b17dbd2e43df>
Function: clean_text at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def clean_text(doc, spacy=True, printed=False, list_tokens=False):
     2                                               '''
     3                                               define a simple preprocessing pipeline for general NLP tasks.
     4                                               non-spaCy version:
     5                                                 - tokenize: split texts into individual tokens
     6                                                 - lowercase: normalize the vocabulary by case
     7                                                 - stopword removal: remove tokens if they appear in a specified list
     8                                                 - tag: tag part of speech (for lemmatization)
     9                                                 - lemmatize: normalize the vocabulary to the base form of each token
    10                                                 - join (optional): return the list of tokens in one joined string
    11                                                 
    12                                                 the spaCy pipeline includes all of this, plus dependency parsing and NER:
    13                                                 
    14                                               spaCy version: (we will disable some of these pipes in testing.)
    15                                                 - tokenize
    16                                                 - lemmatize
    17                                                 - stopword removal
    18                                                 - tag: tag part of speech
    19                                                 - parse: perform dependency parsing
    20                                                 - named entity recognition: extract named entities according to statistical model
    21                                                 - join (optional)
    22                                               '''  
    23                                           
    24     15000      25002.0      1.7      0.4      if spacy:
    25     15000      23089.0      1.5      0.4          try: 
    26     15000    1635546.0    109.0     27.1              doc = [token.lemma_ for token in doc if not token.is_stop and not token.pos_ in ['PRON', 'PUNCT']]
    27                                                       #dropwhile(lambda x: not (x.is_stop and x.pos_ in ['PRON', 'PUNCT']), tokens)
    28                                                       #if not token.is_stop and not token.pos_ in set(['PRON', 'PUNCT'])
    29     15000      26034.0      1.7      0.4              if not list_tokens:
    30     15000    4331156.0    288.7     71.7                  return nlp.make_doc(' '.join([token for token in doc]))
    31                                                       else:
    32                                                           return doc
    33                                                   except AttributeError as ae:
    34                                                       print(f'''ERROR! if parameter spacy == True, corpus input must be of type spacy.tokens.doc.Doc, not {doc.__class__}!\ne.g.: clean_text(nlp("You keep using that word. I do not think it means what you think it means.")''')
    35                                                       raise
    36                                           
    37                                               else:
    38                                                   '''
    39                                                   spaCy's default pipeline includes tokenizer + lemmatizer + POS-tagging
    40                                                   we've added stopword removal to both processes
    41                                                   '''
    42                                                   def pos_tag_nltk(token, printed=False):
    43                                                       tag_map = defaultdict(lambda : wn.NOUN)
    44                                                       tag_map['J'] = wn.ADJ
    45                                                       tag_map['V'] = wn.VERB
    46                                                       tag_map['R'] = wn.ADV
    47                                                   
    48                                                       nonlocal lemmatizer
    49                                                   
    50                                                       token, tag = zip(*pos_tag([token]))
    51                                                       lemma = lemmatizer.lemmatize(token[0], tag_map[tag[0][0]])
    52                                                       if printed:
    53                                                           print(token[0], "=>", lemma)
    54                                                       return lemma
    55                                           
    56                                                   new_doc = []
    57                                                   tokenizer = RegexpTokenizer(r'\w+')
    58                                                   lemmatizer = WordNetLemmatizer()
    59                                                   doc = tokenizer.tokenize(doc)
    60                                                   for token in doc:
    61                                                       if token.lower() not in stop_words:
    62                                                           new_doc.append(pos_tag_nltk(token.lower(), printed))
    63                                                   if not list_tokens:
    64                                                       return ' '.join([token for token in new_doc])
    65                                                   else:
    66                                                       return new_doc