Timer unit: 1e-06 s

Total time: 334.677 s
File: <ipython-input-96-b17dbd2e43df>
Function: clean_text at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def clean_text(doc, spacy=True, printed=False, list_tokens=False):
     2                                               '''
     3                                               define a simple preprocessing pipeline for general NLP tasks.
     4                                               non-spaCy version:
     5                                                 - tokenize: split texts into individual tokens
     6                                                 - lowercase: normalize the vocabulary by case
     7                                                 - stopword removal: remove tokens if they appear in a specified list
     8                                                 - tag: tag part of speech (for lemmatization)
     9                                                 - lemmatize: normalize the vocabulary to the base form of each token
    10                                                 - join (optional): return the list of tokens in one joined string
    11                                                 
    12                                                 the spaCy pipeline includes all of this, plus dependency parsing and NER:
    13                                                 
    14                                               spaCy version: (we will disable some of these pipes in testing.)
    15                                                 - tokenize
    16                                                 - lemmatize
    17                                                 - stopword removal
    18                                                 - tag: tag part of speech
    19                                                 - parse: perform dependency parsing
    20                                                 - named entity recognition: extract named entities according to statistical model
    21                                                 - join (optional)
    22                                               '''  
    23                                           
    24     15000      20910.0      1.4      0.0      if spacy:
    25                                                   try: 
    26                                                       doc = [token.lemma_ for token in doc if not token.is_stop and not token.pos_ in ['PRON', 'PUNCT']]
    27                                                       #dropwhile(lambda x: not (x.is_stop and x.pos_ in ['PRON', 'PUNCT']), tokens)
    28                                                       #if not token.is_stop and not token.pos_ in set(['PRON', 'PUNCT'])
    29                                                       if not list_tokens:
    30                                                           return nlp.make_doc(' '.join([token for token in doc]))
    31                                                       else:
    32                                                           return doc
    33                                                   except AttributeError as ae:
    34                                                       print(f'''ERROR! if parameter spacy == True, corpus input must be of type spacy.tokens.doc.Doc, not {doc.__class__}!\ne.g.: clean_text(nlp("You keep using that word. I do not think it means what you think it means.")''')
    35                                                       raise
    36                                           
    37                                               else:
    38                                                   '''
    39                                                   spaCy's default pipeline includes tokenizer + lemmatizer + POS-tagging
    40                                                   we've added stopword removal to both processes
    41                                                   '''
    42     15000      23537.0      1.6      0.0          def pos_tag_nltk(token, printed=False):
    43                                                       tag_map = defaultdict(lambda : wn.NOUN)
    44                                                       tag_map['J'] = wn.ADJ
    45                                                       tag_map['V'] = wn.VERB
    46                                                       tag_map['R'] = wn.ADV
    47                                                   
    48                                                       nonlocal lemmatizer
    49                                                   
    50                                                       token, tag = zip(*pos_tag([token]))
    51                                                       lemma = lemmatizer.lemmatize(token[0], tag_map[tag[0][0]])
    52                                                       if printed:
    53                                                           print(token[0], "=>", lemma)
    54                                                       return lemma
    55                                           
    56     15000      19268.0      1.3      0.0          new_doc = []
    57     15000      96278.0      6.4      0.0          tokenizer = RegexpTokenizer(r'\w+')
    58     15000      38856.0      2.6      0.0          lemmatizer = WordNetLemmatizer()
    59     15000     736213.0     49.1      0.2          doc = tokenizer.tokenize(doc)
    60   1402734    2250692.0      1.6      0.7          for token in doc:
    61   1387734    5411304.0      3.9      1.6              if token.lower() not in stop_words:
    62    707624  325883634.0    460.5     97.4                  new_doc.append(pos_tag_nltk(token.lower(), printed))
    63     15000      20185.0      1.3      0.0          if not list_tokens:
    64     15000     175849.0     11.7      0.1              return ' '.join([token for token in new_doc])
    65                                                   else:
    66                                                       return new_doc

Total time: 340.878 s
File: <ipython-input-99-6c0116cbe4d8>
Function: nltk_clean_nlp at line 12

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    12                                           def nltk_clean_nlp(docs, spacy=False):
    13         1          1.0      1.0      0.0      new_docs = []
    14     15001      15631.0      1.0      0.0      for doc in docs:
    15     15000  340847755.0  22723.2    100.0          clean_text(doc, spacy=False)
    16     15000      14186.0      0.9      0.0          new_docs.append(doc)
    17                                                   
    18                                                   # if spacy == True:
    19                                                   #     doc = nlp(doc)
    20                                                   #     ents = doc.ents
    21                                                   #     print(doc, ents)
    22                                               
    23         1          0.0      0.0      0.0      return new_docs